{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490ca145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "# Set the main path to the \"Data\" folder\n",
    "main_path = r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data'\n",
    "os.chdir(main_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70230d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to PEAPQ\\PEAPQ_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# PEAPQ\n",
    "\n",
    "# Step 1: Find the CSV file in the PEAPQ folder using wildcards\n",
    "peapq_folder = 'PEAPQ'\n",
    "file_pattern = os.path.join(peapq_folder, 'PEAP-Q+-+PARC*_*.*.csv')\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "if not file_list:\n",
    "    print(\"No files found matching the pattern\")\n",
    "else:\n",
    "    # Assuming the first matching file is the one we want\n",
    "    csv_file = file_list[0]\n",
    "\n",
    "    # Step 2: Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Step 3: Filter out rows without subject information and only include subject numbers 1610 or greater\n",
    "    df = df.dropna(subset=['subject#'])\n",
    "    df['subject#'] = pd.to_numeric(df['subject#'], errors='coerce')  # Convert subject# to numeric\n",
    "    df = df[df['subject#'] >= 1610]\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No valid rows for analysis\")\n",
    "    else:\n",
    "        # Step 4: Calculate the average of the three columns for each subject\n",
    "        # Ensure the subject number column is treated as a categorical variable for proper grouping\n",
    "        df['subject#'] = df['subject#'].astype('category')\n",
    "        df['PEAPQ_SelfRateProf'] = df[['Self Rate Prof_1', 'Self Rate Prof_2', 'Self Rate Prof_3']].mean(axis=1)\n",
    "\n",
    "        # Step 5: Create a summary DataFrame\n",
    "        summary_df = df[['subject#', 'PEAPQ_SelfRateProf']].copy()\n",
    "        summary_df.rename(columns={'subject#': 'Subject'}, inplace=True)\n",
    "\n",
    "        # Optionally, save this summary as a new CSV file or update an existing summary sheet\n",
    "        summary_csv_path = os.path.join(peapq_folder, 'PEAPQ_Summary.csv')\n",
    "        summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "        print(f\"Summary saved to {summary_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e22a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to LEAPQ\\LEAPQ_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# LEAPQ\n",
    "\n",
    "\n",
    "# Set the main path to the \"Data\" folder\n",
    "#main_path = r'C:\\Users\\Maylaka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data'\n",
    "#os.chdir(main_path)\n",
    "\n",
    "# Step 1: Find the CSV file in the LEAPQ folder using wildcards\n",
    "leapq_folder = 'LEAPQ'\n",
    "file_pattern = os.path.join(leapq_folder, 'ALEAP-Q+Short+-+PARC_*')\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "if not file_list:\n",
    "    print(\"No files found matching the pattern\")\n",
    "else:\n",
    "    # Assuming the first matching file is the one we want\n",
    "    csv_file = file_list[0]\n",
    "\n",
    "    # Step 2: Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Step 3: Check values in \"langs order_1\" and \"langs order_2\" for language classification\n",
    "    df['Language Classification'] = 'Monolingual'  # Default value\n",
    "\n",
    "    # Monolingual case\n",
    "    mono_condition = df['langs order_1'].str.isalpha() & df['langs order_2'].str.isalpha()\n",
    "    df.loc[mono_condition, 'Language Classification'] = 'Monolingual'\n",
    "\n",
    "    # Early Bilingual, Late Bilingual, Other cases\n",
    "    df['lang2 age acquire'] = pd.to_numeric(df['lang2 age acquire'], errors='coerce')  # Convert to numeric\n",
    "    early_condition = (df['lang2 age acquire'] < 7) & (df[['lang2 prof rating_1', 'lang2 prof rating_2', 'lang2 prof rating_3']].mean(axis=1, skipna=True) > 7)\n",
    "    late_condition = (df['lang2 age acquire'] >= 7) & (df[['lang2 prof rating_1', 'lang2 prof rating_2', 'lang2 prof rating_3']].mean(axis=1, skipna=True) > 7)\n",
    "\n",
    "    df.loc[early_condition, 'Language Classification'] = 'Early Bilingual'\n",
    "    df.loc[late_condition, 'Language Classification'] = 'Late Bilingual'\n",
    "    df.loc[~(early_condition | late_condition | mono_condition), 'Language Classification'] = 'Other'\n",
    "\n",
    "    # Step 4: Calculate LEAPQ_EngProf based on English presence\n",
    "    df['LEAPQ_EngProf'] = 'manual check needed'  # Default value\n",
    "\n",
    "    eng1_condition = df['langs order_1'].str.contains('English', case=False, na=False)\n",
    "    eng2_condition = df['langs order_2'].str.contains('English', case=False, na=False)\n",
    "    eng3_condition = df['langs order_3'].str.contains('English', case=False, na = False)\n",
    "\n",
    "    df.loc[eng1_condition, 'LEAPQ_EngProf'] = df[['lang1 prof rating_1', 'lang1 prof rating_2', 'lang1 prof rating_3']].mean(axis=1, skipna=True)\n",
    "    df.loc[eng2_condition, 'LEAPQ_EngProf'] = df[['lang2 prof rating_1', 'lang2 prof rating_2', 'lang2 prof rating_3']].mean(axis=1, skipna=True)\n",
    "    df.loc[eng3_condition, 'LEAPQ_EngProf'] = df[['lang2 prof rating_1', 'lang2 prof rating_2', 'lang2 prof rating_3']].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Filter rows based on subject number\n",
    "    df['Subject ID'] = pd.to_numeric(df['Subject ID'], errors='coerce')  # Convert to numeric\n",
    "    df = df[df['Subject ID'].notna()]  # Exclude rows without a subject number\n",
    "    df = df[df['Subject ID'] >= 1610]  # Filter subject numbers >= 1610\n",
    "   \n",
    "\n",
    "    # Step 5: Create a summary DataFrame\n",
    "    summary_df = df[['Subject ID', 'LEAPQ_EngProf']].copy()\n",
    "    summary_df.rename(columns={'Subject ID': 'Subject'}, inplace=True)\n",
    "\n",
    "    # Optionally, save this summary as a new CSV file or update an existing summary sheet\n",
    "    summary_csv_path = os.path.join(leapq_folder, 'LEAPQ_Summary.csv')\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"Summary saved to {summary_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6fa46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics Summary saved to Demographics\\Demographics_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Demographics\n",
    "\n",
    "# Step 1: Find the CSV file in the Demographics folder using wildcards\n",
    "demo_folder = 'Demographics'\n",
    "demo_file_pattern = os.path.join(demo_folder, 'PARCfMRI+Consent+Form+and+Demographics_*')\n",
    "demo_file_list = glob.glob(demo_file_pattern)\n",
    "\n",
    "if not demo_file_list:\n",
    "    print(\"No files found matching the pattern\")\n",
    "else:\n",
    "    # Assuming the first matching file is the one we want\n",
    "    demo_csv_file = demo_file_list[0]\n",
    "\n",
    "    # Step 2: Load the CSV file\n",
    "    demo_df = pd.read_csv(demo_csv_file)\n",
    "\n",
    "    # Step 3: Filter subjects with number equal to or greater than 1610\n",
    "    demo_df['Subject #'] = pd.to_numeric(demo_df['Subject #'], errors='coerce')  # Convert to numeric\n",
    "    demo_df = demo_df[demo_df['Subject #'] >= 1610]\n",
    "\n",
    "    # Step 4: Extract relevant columns\n",
    "    demo_summary_df = demo_df[['Subject #', 'age', 'sex', 'lang classification', 'L1','handedness']].copy()\n",
    "\n",
    "    # Step 5: Rename columns\n",
    "    demo_summary_df.rename(columns={'Subject #': 'Subject', 'age': 'Age', 'sex': 'Sex', 'lang classification': 'Language_ClassificationDemo', 'L1': 'L1', 'handedness': 'Handedness'}, inplace=True)\n",
    "\n",
    "    # Optionally, save this summary as a new CSV file or update an existing summary sheet\n",
    "    demo_summary_csv_path = os.path.join(demo_folder, 'Demographics_Summary.csv')\n",
    "    demo_summary_df.to_csv(demo_summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"Demographics Summary saved to {demo_summary_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8ac9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ND Comp - Form G Summary saved to ND Comp - Form G\\NDComp_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Nelson Denny \n",
    "\n",
    "# Step 1: Find the CSV file in the ND Comp - Form G folder using wildcards\n",
    "ndcomp_folder = 'ND Comp - Form G'\n",
    "ndcomp_file_pattern = os.path.join(ndcomp_folder, 'Nelson+Denny+Comprehension+-+Version+G+*')\n",
    "ndcomp_file_list = glob.glob(ndcomp_file_pattern)\n",
    "\n",
    "if not ndcomp_file_list:\n",
    "    print(\"No files found matching the pattern\")\n",
    "else:\n",
    "    # Assuming the first matching file is the one we want\n",
    "    ndcomp_csv_file = ndcomp_file_list[0]\n",
    "\n",
    "    # Step 2: Load the CSV file\n",
    "    ndcomp_df = pd.read_csv(ndcomp_csv_file)\n",
    "\n",
    "   # Step 3: Extract relevant columns and filter subjects\n",
    "    ndcomp_summary_df = ndcomp_df[['Subject ID', 'SC0', 'reading rate']].copy()\n",
    "    ndcomp_summary_df = ndcomp_summary_df[ndcomp_summary_df['Subject ID'].notna()]\n",
    "    ndcomp_summary_df['Subject ID'] = ndcomp_summary_df['Subject ID'].astype(int)\n",
    "    ndcomp_summary_df = ndcomp_summary_df[ndcomp_summary_df['Subject ID'] >= 1610]\n",
    "\n",
    "    # Step 4: Rename columns\n",
    "    ndcomp_summary_df.rename(columns={'Subject ID': 'Subject', 'SC0': 'NDComp', 'reading rate': 'NDRR'}, inplace=True)\n",
    "\n",
    "    # Optionally, save this summary as a new CSV file or update an existing summary sheet\n",
    "    ndcomp_summary_csv_path = os.path.join(ndcomp_folder, 'NDComp_Summary.csv')\n",
    "    ndcomp_summary_df.to_csv(ndcomp_summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"ND Comp - Form G Summary saved to {ndcomp_summary_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4dfefaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Block  Condition CondName TopType TopStim BtmType BtmStim  \\\n",
      "0      CardSort_PracBlock          1      Con     Sym   Spade    Text   Spade   \n",
      "1      CardSort_PracBlock          1      Con     Txt   Heart     Sym   Heart   \n",
      "2      CardSort_PracBlock          1      Con     Txt   Spade     Sym   Spade   \n",
      "3      CardSort_PracBlock          1      Con     Sym    Club     Txt    Club   \n",
      "4         CardSort_Block1          1      Con     Sym   Heart     Txt   Heart   \n",
      "...                   ...        ...      ...     ...     ...     ...     ...   \n",
      "11951     CardSort_Block4          2    Incon     Txt    Club     Sum   Heart   \n",
      "11952     CardSort_Block4          1      Con     Sym   Heart     Txt   Heart   \n",
      "11953     CardSort_Block4          2    Incon     Sym    Club     Txt   Heart   \n",
      "11954     CardSort_Block4          1      Con     Sym    Club     Txt    Club   \n",
      "11955     CardSort_Block4          1      Con     Txt   Heart     Sym   Heart   \n",
      "\n",
      "       Resp  Status    RT  SymRespCount  TxtRespCount  IncorrRespCount  \\\n",
      "0         3       1  1144             0             0                0   \n",
      "1         2       1  1307             0             0                0   \n",
      "2         3       1   813             0             0                0   \n",
      "3         1       1  1372             0             0                0   \n",
      "4         2       1  1326             0             0                0   \n",
      "...     ...     ...   ...           ...           ...              ...   \n",
      "11951     2       1   739             7            35                5   \n",
      "11952     2       1  1017             7            35                5   \n",
      "11953     3       2  2119             7            35                6   \n",
      "11954     1       1   777             7            35                6   \n",
      "11955     2       1   669             7            35                6   \n",
      "\n",
      "                                                FileName  \\\n",
      "0      WebTask_CardSort_v2.2023-10-27-2052.data.f6e3d...   \n",
      "1      WebTask_CardSort_v2.2023-10-27-2052.data.f6e3d...   \n",
      "2      WebTask_CardSort_v2.2023-10-27-2052.data.f6e3d...   \n",
      "3      WebTask_CardSort_v2.2023-10-27-2052.data.f6e3d...   \n",
      "4      WebTask_CardSort_v2.2023-10-27-2052.data.f6e3d...   \n",
      "...                                                  ...   \n",
      "11951  WebTask_CardSort_v2.2024-05-10-2342.data.8c36b...   \n",
      "11952  WebTask_CardSort_v2.2024-05-10-2342.data.8c36b...   \n",
      "11953  WebTask_CardSort_v2.2024-05-10-2342.data.8c36b...   \n",
      "11954  WebTask_CardSort_v2.2024-05-10-2342.data.8c36b...   \n",
      "11955  WebTask_CardSort_v2.2024-05-10-2342.data.8c36b...   \n",
      "\n",
      "                                         Id  \n",
      "0      f6e3d15b-81f3-4e8a-8bc4-89205a6923a6  \n",
      "1      f6e3d15b-81f3-4e8a-8bc4-89205a6923a6  \n",
      "2      f6e3d15b-81f3-4e8a-8bc4-89205a6923a6  \n",
      "3      f6e3d15b-81f3-4e8a-8bc4-89205a6923a6  \n",
      "4      f6e3d15b-81f3-4e8a-8bc4-89205a6923a6  \n",
      "...                                     ...  \n",
      "11951  8c36b2b7-d279-47bd-a837-0922fbd24289  \n",
      "11952  8c36b2b7-d279-47bd-a837-0922fbd24289  \n",
      "11953  8c36b2b7-d279-47bd-a837-0922fbd24289  \n",
      "11954  8c36b2b7-d279-47bd-a837-0922fbd24289  \n",
      "11955  8c36b2b7-d279-47bd-a837-0922fbd24289  \n",
      "\n",
      "[11956 rows x 15 columns]\n",
      "Analysis summary has been written to C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\CardSort\\CardSort_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Card Sort\n",
    "\n",
    "# Load in the individual text files of data \n",
    "\n",
    "# Specify the path to your folder containing text files\n",
    "#folder_path = r'C:\\Users\\Maylaka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\CardSort\\experiment_data'\n",
    "folder_path = r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\CardSort\\experiment_data'\n",
    "\n",
    "# Get a list of all text files in the folder\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith('.txt')]\n",
    "\n",
    "\n",
    "#Convert the files to a df\n",
    "\n",
    "# Initialize an empty dataframe to store the data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Define column names for the card sorting data\n",
    "column_names = ['Block', 'Condition', 'CondName', 'TopType', 'TopStim', 'BtmType', 'BtmStim', 'Resp', 'Status', 'RT', \n",
    "               'SymRespCount', 'TxtRespCount', 'IncorrRespCount']\n",
    "\n",
    "# Loop through each file and read it into a dataframe\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Assuming space is the delimiter, you can adjust it accordingly\n",
    "    df = pd.read_csv(file_path, delimiter=' ', names = column_names)\n",
    "    \n",
    "        # Add a new column named 'FileName' with the name of the text file\n",
    "    df['FileName'] = os.path.splitext(file)[0]\n",
    "\n",
    "    \n",
    "    # Append the dataframe to the combined dataframe\n",
    "    #combined_df = combined_df.append(df, ignore_index=True)\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index = True)\n",
    "\n",
    "\n",
    "# Extract the substring after 'data.' and assign it to the 'Subject' column\n",
    "combined_df['Id'] = combined_df['FileName'].str.extract(r'data\\.(.*)')\n",
    "\n",
    "print(combined_df)\n",
    "\n",
    "#Pull the counts of verbal vs visual cs responses into a new df\n",
    "\n",
    "# Initialize an empty dataframe for the results\n",
    "result_df = pd.DataFrame(columns=['Id', 'SymRespCount', 'TxtRespCount', 'IncorrRespCount'])\n",
    "\n",
    "# Group by 'Subject' and find the maximum values for each column\n",
    "max_values = combined_df.groupby('Id')[['SymRespCount', 'TxtRespCount', 'IncorrRespCount']].max().reset_index()\n",
    "\n",
    "\n",
    "# Merge the max_values DataFrame with result_df on 'Subject'\n",
    "#result_df = result_df.append(max_values, ignore_index=True)\n",
    "result_df = pd.concat([result_df, max_values], ignore_index = True)\n",
    "\n",
    "\n",
    "# Load in the survey data to get subject # and IRQ; Perform IRQ groupings\n",
    "\n",
    "# Load the data.csv file\n",
    "data_df = pd.read_csv(r'CardSort\\data.csv')\n",
    "\n",
    "# IRQ Analysis\n",
    "\n",
    "# Mapping of IRQ questions to categories\n",
    "irq_visual_questions = ['IRQ_1', 'IRQ_3', 'IRQ_4', 'IRQ_8', 'IRQ_10', 'IRQ_11', 'IRQ_15', 'IRQ_16', 'IRQ_17', 'IRQ_19', 'IRQ_21', 'IRQ_25']\n",
    "irq_verbal_questions = ['IRQ_2', 'IRQ_5', 'IRQ_6', 'IRQ_7', 'IRQ_9', 'IRQ_12', 'IRQ_13', 'IRQ_14', 'IRQ_18', 'IRQ_20', 'IRQ_22', 'IRQ_23', 'IRQ_24', 'IRQ_26']\n",
    "\n",
    "# Reverse code values for IRQ_20 in irq_verbal_questions\n",
    "reverse_code_mapping = {5: 1, 4: 2, 3: 3, 2: 4, 1: 5}\n",
    "data_df['IRQ_20'] = data_df['IRQ_20'].map(reverse_code_mapping)\n",
    "\n",
    "#print(data_df['IRQ_20'])\n",
    "\n",
    "# Pull out the Coding Specific IRQ questions: 5 (Write - Verbal), 10 (Debug - Visual), 17 (Write - Visual) , 23 (Debug - Verbal)\n",
    "irq_verbal_code = ['IRQ_5', 'IRQ_23']\n",
    "irq_visual_code = ['IRQ_10', 'IRQ_17']\n",
    "\n",
    "# Create new columns for IRQ_visual and IRQ_verbal in data_df\n",
    "data_df['IRQ_visual'] = data_df[irq_visual_questions].mean(axis=1)\n",
    "data_df['IRQ_verbal'] = data_df[irq_verbal_questions].mean(axis=1)\n",
    "data_df['IRQ_visual_code'] = data_df[irq_visual_code].mean(axis=1)\n",
    "data_df['IRQ_verbal_code'] = data_df[irq_verbal_code].mean(axis=1)\n",
    "\n",
    "# Initialize a new column 'Expt_number' with NaN values\n",
    "result_df['Subject'] = float('nan')\n",
    "result_df['IRQ_visual'] = float('nan')\n",
    "result_df['IRQ_verbal'] = float('nan')\n",
    "result_df['IRQ_visual_code'] = float('nan')\n",
    "result_df['IRQ_verbal_code'] = float('nan')\n",
    "\n",
    "# Iterate through each row in result_df\n",
    "for index, row in result_df.iterrows():\n",
    "    # Extract the unique part of the 'Subject' value for partial matching\n",
    "    partial_match = row['Id'].split('.')[0]\n",
    "    \n",
    "    # Perform partial string matching and get the corresponding 'subjnum_1' value\n",
    "    match_row = data_df[data_df['participant'].str.contains(partial_match)]\n",
    "    \n",
    "    # If a match is found, update the 'Expt_number' column in result_df\n",
    "    if not match_row.empty:\n",
    "        result_df.at[index, 'Subject'] = match_row['subjnum_1'].values[0]\n",
    "        result_df.at[index, 'IRQ_visual'] = match_row['IRQ_visual'].values[0]\n",
    "        result_df.at[index, 'IRQ_verbal'] = match_row['IRQ_verbal'].values[0]\n",
    "        result_df.at[index, 'IRQ_visual_code'] = match_row['IRQ_visual_code'].values[0]\n",
    "        result_df.at[index, 'IRQ_verbal_code'] = match_row['IRQ_verbal_code'].values[0]\n",
    "\n",
    "# Drop the 'Id' column\n",
    "result_df = result_df.drop(['Id'], axis=1)\n",
    "\n",
    "# Calculate VerbalBiasScore and add it as a new column\n",
    "result_df['VerbalBiasScore'] = (result_df['TxtRespCount'] - result_df['SymRespCount']) / (result_df['TxtRespCount'] + result_df['SymRespCount'])\n",
    "\n",
    "# Filter subjects\n",
    "result_df = result_df[result_df['Subject'].notna()]\n",
    "result_df['Subject'] = pd.to_numeric(result_df['Subject'], errors='coerce')\n",
    "result_df = result_df[result_df['Subject'] >= 1610]\n",
    "\n",
    "# Save the result_df to an analysis summary CSV file\n",
    "csv_file_path = os.path.join(r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\CardSort\\CardSort_Summary.csv')\n",
    "result_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Analysis summary has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370265a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Subject         Id     task  trial direction  setsize  correctresp  \\\n",
      "0          0   336916.0  fortask      1   forward        3          582   \n",
      "1          0   336916.0  fortask      2   forward        3          694   \n",
      "2          0   336916.0  fortask      3   forward        4         6439   \n",
      "3          0   336916.0  fortask      4   forward        4         7286   \n",
      "4          0   336916.0  fortask      5   forward        5        42731   \n",
      "..       ...        ...      ...    ...       ...      ...          ...   \n",
      "737    16330  1373204.0  fortask     10   forward        7      4179386   \n",
      "738    16330  1373204.0  fortask     11   forward        8     58192647   \n",
      "739    16330  1373204.0  fortask     12   forward        8     38295174   \n",
      "740    16330  1373204.0  fortask     13   forward        9    275862584   \n",
      "741    16330  1373204.0  fortask     14   forward        9    713942568   \n",
      "\n",
      "       subresp  Acc     RT  Span  \\\n",
      "0          582    1   3253     0   \n",
      "1          694    1   2934     3   \n",
      "2         6439    1   4475     3   \n",
      "3         7286    1   4250     4   \n",
      "4        42731    1   3929     4   \n",
      "..         ...  ...    ...   ...   \n",
      "737    4139723    2  27974     5   \n",
      "738    5829184    2  15450     5   \n",
      "739   38512748    2  20983     5   \n",
      "740  278654673    2  45814     5   \n",
      "741   74231948    2  20156     5   \n",
      "\n",
      "                                              FileName  \n",
      "0    PARC_ForwardDigitSpan.2024-01-23-0316.data.677...  \n",
      "1    PARC_ForwardDigitSpan.2024-01-23-0316.data.677...  \n",
      "2    PARC_ForwardDigitSpan.2024-01-23-0316.data.677...  \n",
      "3    PARC_ForwardDigitSpan.2024-01-23-0316.data.677...  \n",
      "4    PARC_ForwardDigitSpan.2024-01-23-0316.data.677...  \n",
      "..                                                 ...  \n",
      "737  PARC_ForwardDigitSpan.2024-05-18-0315.data.2a7...  \n",
      "738  PARC_ForwardDigitSpan.2024-05-18-0315.data.2a7...  \n",
      "739  PARC_ForwardDigitSpan.2024-05-18-0315.data.2a7...  \n",
      "740  PARC_ForwardDigitSpan.2024-05-18-0315.data.2a7...  \n",
      "741  PARC_ForwardDigitSpan.2024-05-18-0315.data.2a7...  \n",
      "\n",
      "[742 rows x 12 columns]\n",
      "Analysis summary has been written to C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\Fwd Digit Span\\DigitSpan_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Digit Span\n",
    "\n",
    "# Load in the individual text files of data \n",
    "\n",
    "# Specify the path to your folder containing text files\n",
    "folder_path = r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\Fwd Digit Span\\experiment_data'\n",
    "\n",
    "\n",
    "# Get a list of all text files in the folder\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith('.txt')]\n",
    "\n",
    "#print(files)\n",
    "\n",
    "#Convert the files to a df\n",
    "\n",
    "# Initialize an empty dataframe to store the data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Define column names for the card sorting data\n",
    "column_names = ['Subject', 'Id', 'task', 'trial', 'direction', 'setsize', 'correctresp', 'subresp', 'Acc', 'RT', 'Span']\n",
    "\n",
    "# Loop through each file and read it into a dataframe\n",
    "# Loop through each file and read it into a dataframe\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Assuming space is the delimiter, you can adjust it accordingly\n",
    "    df = pd.read_csv(file_path, delimiter=' ', names = column_names)\n",
    "    \n",
    "        # Add a new column named 'FileName' with the name of the text file\n",
    "    df['FileName'] = os.path.splitext(file)[0]\n",
    "    \n",
    "    # Append the dataframe to the combined dataframe\n",
    "    #combined_df = combined_df.append(df, ignore_index=True)\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "\n",
    "#Pull the span into a new df\n",
    "\n",
    "# Initialize an empty dataframe for the results\n",
    "result_df = pd.DataFrame(columns=['Subject', 'Span'])\n",
    "\n",
    "# Group by 'Subject' and find the maximum values for each column\n",
    "max_values = combined_df.groupby('Subject')[['Span']].max().reset_index()\n",
    "\n",
    "\n",
    "# Merge the max_values DataFrame with result_df on 'Subject'\n",
    "#result_df = result_df.append(max_values, ignore_index=True)\n",
    "result_df = pd.concat([result_df, max_values], ignore_index=True)\n",
    "\n",
    "#Filter subjects \n",
    "result_df = result_df[result_df['Subject'].notna()]\n",
    "result_df['Subject'] = pd.to_numeric(result_df['Subject'], errors='coerce')\n",
    "result_df = result_df[result_df['Subject'] >= 1610]\n",
    "\n",
    "# Save the result_df to an analysis summary CSV file\n",
    "csv_file_path = os.path.join(r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\Fwd Digit Span\\DigitSpan_Summary.csv')\n",
    "result_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Analysis summary has been written to {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ca86b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data has been written to C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\PyTask\\PyTask_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# PyTask\n",
    "\n",
    "\n",
    "# Path to the main folder containing subject folders\n",
    "main_folder_path = r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\PyTask'\n",
    "\n",
    "# List to store the summary data\n",
    "summary_data = []\n",
    "\n",
    "# Loop through each subject folder\n",
    "for subject_folder_name in os.listdir(main_folder_path):\n",
    "    subject_folder_path = os.path.join(main_folder_path, subject_folder_name)\n",
    "\n",
    "    # Check if it's a directory (to skip any files in the main folder)\n",
    "    if os.path.isdir(subject_folder_path):\n",
    "        # Initialize variables for each subject\n",
    "        subject_data = {'Subject': subject_folder_name, 'PY_ACC': 0, 'PY_RT': 0, 'PY_CCAcc': 0, 'PY_CCRT': 0,\n",
    "                        'WR_ACC': 0, 'WR_RT': 0, 'WR_CCAcc': 0, 'WR_CCRT': 0,\n",
    "                        'Count_PY': 0, 'Count_WR': 0}\n",
    "\n",
    "        # Loop through each condition (PY and WR)\n",
    "        for condition in ['PY', 'WR']:\n",
    "            combined_df = pd.DataFrame()  # Combined dataframe for each condition\n",
    "\n",
    "            # Loop through the 4 relevant text files for each condition\n",
    "            for file_name in os.listdir(subject_folder_path):\n",
    "                if file_name.startswith(f'{condition}SubjectResponse'):\n",
    "                    file_path = os.path.join(subject_folder_path, file_name)\n",
    "\n",
    "                    # Read the text file into a dataframe\n",
    "                    df = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "                    # Append the dataframe to the combined_df\n",
    "                    #combined_df = combined_df.append(df, ignore_index=True)\n",
    "                    combined_df = pd.concat([combined_df, df], ignore_index = True)\n",
    "\n",
    "            # Calculate the average ACC, RT, CCAcc, and CCRT for the condition\n",
    "            if not combined_df.empty:\n",
    "                subject_data[f'{condition}_ACC'] = combined_df['Acc'].mean()\n",
    "                subject_data[f'{condition}_RT'] = combined_df['RT'].mean()\n",
    "                subject_data[f'{condition}_CCAcc'] = combined_df['CCAcc'].mean()\n",
    "                subject_data[f'{condition}_CCRT'] = combined_df['CCRT'].mean()\n",
    "                subject_data[f'Count_{condition}'] = len(combined_df)\n",
    "\n",
    "        # Append the subject_data to the summary_data list\n",
    "        summary_data.append(subject_data)\n",
    "        #summary_data = pd.concat([summary_data, subject_data], ignore_index = True)\n",
    "\n",
    "# Write the summary_data to a CSV file\n",
    "csv_file_path = r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\PyTask\\PyTask_Summary.csv'\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    fieldnames = ['Subject', 'PY_ACC', 'PY_RT', 'PY_CCAcc', 'PY_CCRT', 'Count_PY', 'WR_ACC', 'WR_RT', 'WR_CCAcc', 'WR_CCRT', 'Count_WR']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for row in summary_data:\n",
    "        # Exclude additional fields not in fieldnames\n",
    "        row_to_write = {key: row[key] for key in fieldnames}\n",
    "        writer.writerow(row_to_write)\n",
    "\n",
    "print(f\"Summary data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe6ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary has been written to Localizer/Localizer_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Localizer\n",
    "\n",
    "# Path to the main folder containing subject folders\n",
    "main_folder_path = r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\Localizer'\n",
    "\n",
    "\n",
    "# List to store the combined data\n",
    "combined_data = []\n",
    "\n",
    "# Loop through each subject folder\n",
    "for subject_folder_name in os.listdir(main_folder_path):\n",
    "    subject_folder_path = os.path.join(main_folder_path, subject_folder_name)\n",
    "\n",
    "    # Check if it's a directory (to skip any files in the main folder)\n",
    "    if os.path.isdir(subject_folder_path):\n",
    "        # Initialize variables for each subject\n",
    "        subject_data = []\n",
    "\n",
    "        # Loop through the SubjectResponse* files in the subject folder\n",
    "        for file_name in os.listdir(subject_folder_path):\n",
    "            if file_name.startswith('SubjectResponse'):\n",
    "                file_path = os.path.join(subject_folder_path, file_name)\n",
    "                #print(f\"Reading file: {file_path}\")\n",
    "\n",
    "                # Read the text file into a DataFrame\n",
    "                df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=['Run', 'Condition', 'Stim 1', 'Stim 2', 'Stim 3', 'Stim 4', 'Stim 5', 'Stim 6'])\n",
    "                \n",
    "                # Add the 'Subject' column to the DataFrame\n",
    "                df.insert(0, 'Subject', subject_folder_name)\n",
    "\n",
    "                # Append the DataFrame to the subject_data\n",
    "                subject_data.append(df)\n",
    "\n",
    "        # Check if any SubjectResponse* files were found\n",
    "        if subject_data:\n",
    "            # Concatenate the DataFrames for this subject\n",
    "            subject_df = pd.concat(subject_data, ignore_index=True)\n",
    "\n",
    "            # Calculate average accuracy for RJ and LS conditions\n",
    "            rj_accuracy = subject_df.loc[subject_df['Condition'] == 'RJ', 'Stim 1':'Stim 6'].mean().mean()\n",
    "            ls_accuracy = subject_df.loc[subject_df['Condition'] == 'LS', 'Stim 1':'Stim 6'].mean().mean()\n",
    "\n",
    "            # Append results to combined_data\n",
    "            combined_data.append({'Subject': subject_folder_name, 'RJ_Acc': rj_accuracy, 'LS_Acc': ls_accuracy})\n",
    "\n",
    "# Check if any combined data was found\n",
    "if combined_data:\n",
    "    # Create results_summary DataFrame\n",
    "    results_summary = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Save the results_summary to a CSV file\n",
    "    csv_file_path = 'Localizer/Localizer_Summary.csv'\n",
    "    results_summary.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    print(f\"Results summary has been written to {csv_file_path}\")\n",
    "else:\n",
    "    print(\"No SubjectResponse* files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ff56f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Subject   Age     Sex Language_ClassificationDemo          L1  \\\n",
      "0    1610.0  19.0    Male             Early Bilingual     English   \n",
      "1    1611.0  21.0    Male                 Monolingual    Chinese    \n",
      "2    1612.0  22.0  Female              Late Bilingual    Mandarin   \n",
      "3    1619.0  21.0  Female             Early Bilingual     Chinses   \n",
      "4    1615.0  30.0  Female                 Monolingual     English   \n",
      "5    1616.0  20.0  Female                 Monolingual     English   \n",
      "6    1613.0  26.0    Male             Early Bilingual     English   \n",
      "7    1617.0  19.0    Male              Late Bilingual     English   \n",
      "8    1614.0  20.0  Female              Late Bilingual     English   \n",
      "9    1620.0  25.0    Male             Early Bilingual   Taiwanese   \n",
      "10   1618.0  24.0    Male              Late Bilingual  Indonesian   \n",
      "11   1621.0  19.0    Male                 Monolingual     English   \n",
      "12   1623.0  21.0    Male             Early Bilingual   Cantonese   \n",
      "13   1622.0  19.0    Male              Late Bilingual     English   \n",
      "14   1629.0  23.0  Female              Late Bilingual     Chinese   \n",
      "15   1625.0  21.0    Male             Early Bilingual     English   \n",
      "16   1634.0  28.0    Male             Early Bilingual     English   \n",
      "17   1630.0  20.0  Female                 Monolingual     English   \n",
      "18   1628.0  20.0  Female                 Monolingual     English   \n",
      "19   1631.0  24.0    Male             Early Bilingual     English   \n",
      "20   1626.0  19.0    Male             Early Bilingual     Punjabi   \n",
      "21   1624.0  24.0  Female             Early Bilingual    Mandarin   \n",
      "22   1627.0  23.0    Male             Early Bilingual   Gujarathi   \n",
      "23   1633.0  21.0    Male             Early Bilingual     English   \n",
      "24   1635.0  29.0  Female             Early Bilingual     English   \n",
      "25   1643.0  18.0    Male                 Monolingual     English   \n",
      "26   1644.0  25.0  Female              Late Bilingual      Korean   \n",
      "27   1641.0  33.0    Male              Late Bilingual     English   \n",
      "28   1638.0  25.0    Male                 Monolingual     English   \n",
      "29   1640.0  35.0    Male             Early Bilingual     Punjabi   \n",
      "30   1645.0  20.0    Male           None of the above     English   \n",
      "31   1639.0  20.0    Male                 Monolingual     English   \n",
      "32   1642.0  18.0    Male              Late Bilingual     English   \n",
      "33   1650.0  19.0  Female             Early Bilingual     English   \n",
      "34   1649.0  25.0  Female             Early Bilingual     Chinese   \n",
      "35   1648.0  18.0    Male              Late Bilingual     Chinese   \n",
      "36   1651.0  19.0    Male              Late Bilingual     Chinese   \n",
      "37   1656.0  20.0  Female             Early Bilingual     English   \n",
      "38   1632.0  35.0  Female             Early Bilingual     English   \n",
      "39   1637.0  28.0    Male                 Monolingual     English   \n",
      "40   1636.0  22.0    Male              Late Bilingual     Chinese   \n",
      "41   1646.0  21.0    Male              Late Bilingual     English   \n",
      "42   1655.0  21.0    Male              Late Bilingual     Chinese   \n",
      "43   1652.0  20.0  Female             Early Bilingual  Vietnamese   \n",
      "44   1657.0  31.0    Male             Early Bilingual     Bengali   \n",
      "45   1647.0  28.0    Male                 Monolingual     English   \n",
      "46   1658.0  18.0  Female              Late Bilingual     English   \n",
      "47   1654.0  23.0  Female             Early Bilingual     Chinese   \n",
      "48   1653.0  29.0    Male              Late Bilingual     Chinese   \n",
      "\n",
      "       Handedness        LEAPQ_EngProf  NDComp   NDRR  PEAPQ_SelfRateProf  \\\n",
      "0           Right                 10.0    32.0  355.0            3.333333   \n",
      "1            Left    8.333333333333334    27.0  232.0            3.333333   \n",
      "2           Right    8.666666666666666    25.0  247.0            2.000000   \n",
      "3           Right                  8.0    29.0  415.0            7.333333   \n",
      "4           Right  manual check needed    30.0  137.0            4.000000   \n",
      "5           Right    9.666666666666666    36.0  261.0            4.333333   \n",
      "6           Right    9.666666666666666    35.0  261.0            7.333333   \n",
      "7           Right    7.666666666666667    30.0  179.0            7.333333   \n",
      "8           Right                 10.0    31.0  535.0            6.000000   \n",
      "9   No preference                 10.0    34.0  601.0            3.000000   \n",
      "10          Right    8.666666666666666    32.0  223.0            8.000000   \n",
      "11          Right                 10.0    35.0  443.0            7.666667   \n",
      "12          Right                 10.0    25.0  179.0            6.666667   \n",
      "13          Right                 10.0    32.0  299.0            5.666667   \n",
      "14          Right                  5.0    14.0   56.0            4.666667   \n",
      "15          Right    8.666666666666666    32.0  179.0            5.000000   \n",
      "16          Right    9.333333333333334    32.0  299.0            6.333333   \n",
      "17          Right                 10.0    26.0  232.0            2.666667   \n",
      "18          Right                 10.0    25.0  299.0            2.000000   \n",
      "19          Right    9.666666666666666    31.0  211.0            8.000000   \n",
      "20          Right                  8.0    30.0  164.0            4.333333   \n",
      "21          Right                 10.0    37.0  382.0            3.000000   \n",
      "22          Right    8.666666666666666    31.0  232.0            5.666667   \n",
      "23          Right                 10.0    34.0  325.0            3.333333   \n",
      "24          Right                 10.0    35.0  456.0           10.000000   \n",
      "25          Right                 10.0    36.0  279.0            6.333333   \n",
      "26          Right    8.666666666666666    11.0  232.0            6.000000   \n",
      "27          Right                 10.0    29.0  261.0            7.333333   \n",
      "28          Right                 10.0    34.0  211.0            8.000000   \n",
      "29          Right    9.666666666666666    35.0  299.0            9.000000   \n",
      "30          Right                 10.0    28.0  164.0            5.000000   \n",
      "31          Right                 10.0    33.0  339.0            7.333333   \n",
      "32          Right                  9.0    34.0  325.0            4.666667   \n",
      "33          Right                 10.0    35.0  211.0            5.000000   \n",
      "34          Right                 10.0    34.0  473.0            2.333333   \n",
      "35          Right    9.666666666666666    34.0  232.0            3.000000   \n",
      "36          Right    6.666666666666667    24.0  233.0            6.000000   \n",
      "37          Right                 10.0    27.0  195.0            2.000000   \n",
      "38          Right                 10.0    36.0  299.0            5.000000   \n",
      "39          Right                 10.0    32.0  312.0            7.000000   \n",
      "40          Right    6.666666666666667    17.0  179.0            6.000000   \n",
      "41          Right                 10.0    35.0  443.0            5.000000   \n",
      "42          Right                  6.0    21.0  154.0            5.000000   \n",
      "43          Right                  7.0    28.0  279.0            4.000000   \n",
      "44          Right                 10.0    25.0  279.0            9.000000   \n",
      "45          Right                 10.0    34.0  364.0            3.000000   \n",
      "46          Right    9.333333333333334    35.0  415.0            6.666667   \n",
      "47          Right                 10.0     NaN    NaN            4.333333   \n",
      "48          Right                 10.0    29.0  223.0            7.666667   \n",
      "\n",
      "    ...   PY_ACC       PY_RT  PY_CCAcc   PY_CCRT  Count_PY   WR_ACC  \\\n",
      "0   ...  0.78125  6905.09375     0.750  2060.500      32.0  0.96875   \n",
      "1   ...  0.75000  7179.50000     0.875  2870.500      32.0  0.90625   \n",
      "2   ...  0.68750  6389.37500     0.625  1900.375      32.0  0.96875   \n",
      "3   ...  0.75000  4248.34375     0.500  1892.875      32.0  0.84375   \n",
      "4   ...  0.87500  7042.28125     0.750  1535.750      32.0  1.00000   \n",
      "5   ...  0.78125  6646.25000     0.750  2416.500      32.0  1.00000   \n",
      "6   ...      NaN         NaN       NaN       NaN       NaN      NaN   \n",
      "7   ...  0.90625  4537.84375     0.625  1966.125      32.0  0.90625   \n",
      "8   ...  0.81250  4446.00000     0.625  2157.625      32.0  0.87500   \n",
      "9   ...      NaN         NaN       NaN       NaN       NaN      NaN   \n",
      "10  ...  0.90625  5556.46875     1.000  1958.000      32.0  1.00000   \n",
      "11  ...  0.75000  5066.96875     1.000  2432.500      32.0  0.96875   \n",
      "12  ...  0.96875  6040.43750     0.500  1679.625      32.0  0.93750   \n",
      "13  ...  0.75000  5707.25000     0.750  2223.000      32.0  0.96875   \n",
      "14  ...  0.71875  5666.15625     0.750  2312.750      32.0  0.90625   \n",
      "15  ...  0.68750  6114.12500     0.750  2363.875      32.0  0.96875   \n",
      "16  ...  0.84375  6118.12500     0.750  1343.375      32.0  1.00000   \n",
      "17  ...  0.62500  4710.40625     1.000  1678.125      32.0  1.00000   \n",
      "18  ...  0.78125  7043.84375     0.750  2595.875      32.0  1.00000   \n",
      "19  ...  0.81250  4387.40625     0.875  1777.625      32.0  0.96875   \n",
      "20  ...  0.59375  6256.62500     0.750  1684.750      32.0  0.96875   \n",
      "21  ...  0.75000  6282.53125     0.625  1944.625      32.0  1.00000   \n",
      "22  ...  0.71875  7484.87500     0.750  2176.625      32.0  0.96875   \n",
      "23  ...  0.71875  4872.93750     0.750  1757.500      32.0  1.00000   \n",
      "24  ...  0.90625  4296.18750     0.875  1233.750      32.0  1.00000   \n",
      "25  ...  0.71875  5949.62500     0.500  2692.625      32.0  1.00000   \n",
      "26  ...  0.84375  6340.28125     0.875  2214.875      32.0  0.96875   \n",
      "27  ...  0.84375  4928.06250     0.750  1507.875      32.0  1.00000   \n",
      "28  ...  0.78125  5382.53125     0.750  1671.750      32.0  0.93750   \n",
      "29  ...  0.81250  6213.25000     0.750  1755.875      32.0  1.00000   \n",
      "30  ...  0.90625  7375.43750     0.875  2662.875      32.0  1.00000   \n",
      "31  ...  0.87500  6591.53125     0.875  2243.500      32.0  0.96875   \n",
      "32  ...  0.78125  4383.81250     1.000  1074.125      32.0  1.00000   \n",
      "33  ...  0.68750  5805.53125     0.375  3063.875      32.0  1.00000   \n",
      "34  ...  0.56250  6279.59375     0.500  2111.375      32.0  0.96875   \n",
      "35  ...  0.81250  6558.37500     0.750  1677.000      32.0  1.00000   \n",
      "36  ...  0.87500  5294.81250     0.875  1501.875      32.0  0.90625   \n",
      "37  ...  0.53125  6101.65625     0.500  2195.500      32.0  0.93750   \n",
      "38  ...  0.78125  6275.28125     0.750  2030.375      32.0  1.00000   \n",
      "39  ...  0.84375  6404.96875     1.000  2044.500      32.0  1.00000   \n",
      "40  ...  0.75000  7171.62500     0.625  1447.000      32.0  0.59375   \n",
      "41  ...      NaN         NaN       NaN       NaN       NaN      NaN   \n",
      "42  ...  0.87500  6087.78125     0.875  1126.125      32.0  0.96875   \n",
      "43  ...  0.84375  6906.00000     0.875  2707.750      32.0  0.93750   \n",
      "44  ...  0.78125  4690.28125     0.750  1330.625      32.0  0.96875   \n",
      "45  ...  0.68750  6326.84375     0.750  1909.250      32.0  0.96875   \n",
      "46  ...  0.71875  5195.37500     0.875  1410.750      32.0  0.96875   \n",
      "47  ...  0.81250  5991.75000     1.000  1742.875      32.0  1.00000   \n",
      "48  ...  0.81250  6673.68750     0.875  1965.625      32.0  0.84375   \n",
      "\n",
      "         WR_RT  WR_CCAcc   WR_CCRT  Count_WR  \n",
      "0   2377.15625     0.875  1368.125      32.0  \n",
      "1   4672.00000     0.875  2198.375      32.0  \n",
      "2   3204.15625     1.000  1819.125      32.0  \n",
      "3   3213.81250     0.875  1318.250      32.0  \n",
      "4   3163.96875     1.000  1266.625      32.0  \n",
      "5   2864.53125     1.000  1575.500      32.0  \n",
      "6          NaN       NaN       NaN       NaN  \n",
      "7   2521.62500     1.000  1651.625      32.0  \n",
      "8   1880.56250     1.000  1659.500      32.0  \n",
      "9          NaN       NaN       NaN       NaN  \n",
      "10  3300.68750     0.625  1283.000      32.0  \n",
      "11  2808.31250     0.750  1487.375      32.0  \n",
      "12  3119.93750     0.625  1403.375      32.0  \n",
      "13  3418.03125     0.875  1685.250      32.0  \n",
      "14  3254.93750     0.500  1403.375      32.0  \n",
      "15  4265.37500     0.750  2017.500      32.0  \n",
      "16  2822.31250     1.000  1574.750      32.0  \n",
      "17  2576.06250     0.875  1262.500      32.0  \n",
      "18  3034.25000     1.000  1800.875      32.0  \n",
      "19  2352.12500     1.000  1554.125      32.0  \n",
      "20  2420.43750     1.000  1455.125      32.0  \n",
      "21  2907.62500     0.750  1933.125      32.0  \n",
      "22  4220.53125     0.750  2673.625      32.0  \n",
      "23  2225.93750     1.000  1641.625      32.0  \n",
      "24  1715.31250     0.875  1099.875      32.0  \n",
      "25  2404.12500     1.000  1305.375      32.0  \n",
      "26  3126.12500     0.875  1727.750      32.0  \n",
      "27  3725.71875     1.000  1833.000      32.0  \n",
      "28  4716.62500     1.000  2008.750      32.0  \n",
      "29  3227.46875     0.750  2184.875      32.0  \n",
      "30  3218.15625     0.875  1824.500      32.0  \n",
      "31  3349.84375     0.875  1919.875      32.0  \n",
      "32  1598.50000     1.000  1037.250      32.0  \n",
      "33  1603.21875     0.750  1075.750      32.0  \n",
      "34  2722.18750     0.875  2168.000      32.0  \n",
      "35  3618.12500     0.875  1712.375      32.0  \n",
      "36  3518.59375     0.875  2315.125      32.0  \n",
      "37  3944.65625     0.750  1258.750      32.0  \n",
      "38  2930.09375     1.000  2462.875      32.0  \n",
      "39  3608.78125     1.000  1708.750      32.0  \n",
      "40  5031.25000     1.000  1799.625      32.0  \n",
      "41         NaN       NaN       NaN       NaN  \n",
      "42  3201.00000     1.000  1905.625      32.0  \n",
      "43  5040.96875     1.000  2650.750      32.0  \n",
      "44  3185.06250     0.750  2068.250      32.0  \n",
      "45  2498.65625     1.000  1201.250      32.0  \n",
      "46  2500.96875     0.750  1914.625      32.0  \n",
      "47  3777.31250     0.875  1532.250      32.0  \n",
      "48  3856.28125     0.875  1672.250      32.0  \n",
      "\n",
      "[49 rows x 31 columns]\n",
      "Analysis summary has been written to C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data\\Analysis_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Combining it all into one summary sheet!\n",
    "\n",
    "# Set the base path\n",
    "base_path = r'C:\\Users\\Malayka Mottarella\\OneDrive - UW\\CCDL\\CCDL Shared\\Expt\\PARC\\Data'\n",
    "\n",
    "# Load Demographics_Summary\n",
    "demographics_path = os.path.join(base_path, 'Demographics', 'Demographics_Summary.csv')\n",
    "demographics_df = pd.read_csv(demographics_path)\n",
    "\n",
    "# Load LEAPQ_Summary\n",
    "leapq_path = os.path.join(base_path, 'LEAPQ', 'LEAPQ_Summary.csv')\n",
    "leapq_df = pd.read_csv(leapq_path)\n",
    "\n",
    "# Load NDComp_Summary\n",
    "ndcomp_path = os.path.join(base_path, 'ND Comp - Form G', 'NDComp_Summary.csv')\n",
    "ndcomp_df = pd.read_csv(ndcomp_path)\n",
    "\n",
    "# Load PEAPQ_Summary\n",
    "peapq_path = os.path.join(base_path, 'PEAPQ', 'PEAPQ_Summary.csv')\n",
    "peapq_df = pd.read_csv(peapq_path)\n",
    "\n",
    "# Load Digit Span\n",
    "span_path = os.path.join(base_path, 'Fwd Digit Span', 'DigitSpan_Summary.csv')\n",
    "span_df = pd.read_csv(span_path)\n",
    "\n",
    "# Load Card Sort Summary\n",
    "cardsort_path = os.path.join(base_path, 'CardSort', 'CardSort_Summary.csv')\n",
    "cardsort_df = pd.read_csv(cardsort_path)\n",
    "\n",
    "# Load Localizer Summary\n",
    "loc_path = os.path.join(base_path, 'Localizer', 'Localizer_Summary.csv')\n",
    "loc_df = pd.read_csv(loc_path)\n",
    "\n",
    "# Load PyTask Summary\n",
    "py_path = os.path.join(base_path, 'PyTask', 'PyTask_Summary.csv')\n",
    "py_df = pd.read_csv(py_path)\n",
    "\n",
    "\n",
    "# Merge dataframes based on the \"Subject\" column\n",
    "merged_df = pd.merge(demographics_df, leapq_df, how = 'left', on = 'Subject')\n",
    "merged_df = pd.merge(merged_df, ndcomp_df, how = 'left', on = 'Subject')\n",
    "merged_df = pd.merge(merged_df, peapq_df, how = 'left', on='Subject')\n",
    "merged_df = pd.merge(merged_df, span_df, how = 'left', on='Subject')\n",
    "merged_df = pd.merge(merged_df, cardsort_df, how = 'left', on='Subject')\n",
    "merged_df = pd.merge(merged_df, loc_df, how = 'left', on='Subject')\n",
    "merged_df = pd.merge(merged_df, py_df, how = 'left', on='Subject')\n",
    "print(merged_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "summary_csv_path = os.path.join(base_path, 'Analysis_Summary.csv')\n",
    "merged_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"Analysis summary has been written to {summary_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87b279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6d21c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
